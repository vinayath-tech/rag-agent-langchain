import { isBaseMessage } from "@langchain/core/messages";
import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";
import { Client as LangSmithClient, getDefaultProjectName } from "langsmith";
import { getLangGraphCommand } from "./command.mjs";
import { checkLangGraphSemver } from "./semver/index.mjs";
import { runnableConfigToCheckpoint, taskRunnableConfigToCheckpoint, } from "./utils/runnableConfig.mjs";
const isRunnableConfig = (config) => {
    if (typeof config !== "object" || config == null)
        return false;
    return ("configurable" in config &&
        typeof config.configurable === "object" &&
        config.configurable != null);
};
function preprocessDebugCheckpointTask(task) {
    if (!isRunnableConfig(task.state) ||
        !taskRunnableConfigToCheckpoint(task.state)) {
        return task;
    }
    const cloneTask = { ...task };
    cloneTask.checkpoint = taskRunnableConfigToCheckpoint(task.state);
    delete cloneTask.state;
    return cloneTask;
}
const isConfigurablePresent = (config) => typeof config === "object" &&
    config != null &&
    "configurable" in config &&
    typeof config.configurable === "object" &&
    config.configurable != null;
const deleteInternalConfigurableFields = (config) => {
    if (isConfigurablePresent(config)) {
        const newConfig = {
            ...config,
            configurable: Object.fromEntries(Object.entries(config.configurable).filter(([key]) => !key.startsWith("__"))),
        };
        delete newConfig.callbacks;
        return newConfig;
    }
    return config;
};
function preprocessDebugCheckpoint(payload) {
    const result = {
        ...payload,
        checkpoint: runnableConfigToCheckpoint(payload["config"]),
        parent_checkpoint: runnableConfigToCheckpoint(payload["parentConfig"]),
        tasks: payload["tasks"].map(preprocessDebugCheckpointTask),
    };
    // Handle LangGraph JS pascalCase vs snake_case
    // TODO: use stream to LangGraph.JS
    result.parent_config = payload["parentConfig"];
    delete result.parentConfig;
    result.config = deleteInternalConfigurableFields(result.config);
    result.parent_config = deleteInternalConfigurableFields(result.parent_config);
    return result;
}
let LANGGRAPH_VERSION;
export async function* streamState(run, options) {
    const kwargs = run.kwargs;
    const graphId = kwargs.config?.configurable?.graph_id;
    if (!graphId || typeof graphId !== "string") {
        throw new Error("Invalid or missing graph_id");
    }
    const graph = await options.getGraph(graphId, kwargs.config, {
        checkpointer: kwargs.temporary ? null : undefined,
    });
    const userStreamMode = kwargs.stream_mode ?? [];
    const libStreamMode = new Set(userStreamMode.filter((mode) => mode !== "events" && mode !== "messages-tuple") ?? []);
    if (userStreamMode.includes("messages-tuple")) {
        libStreamMode.add("messages");
    }
    if (userStreamMode.includes("messages")) {
        libStreamMode.add("values");
    }
    if (!libStreamMode.has("debug"))
        libStreamMode.add("debug");
    yield {
        event: "metadata",
        data: { run_id: run.run_id, attempt: options.attempt },
    };
    if (!LANGGRAPH_VERSION) {
        const version = await checkLangGraphSemver();
        LANGGRAPH_VERSION = version.find((v) => v.name === "@langchain/langgraph");
    }
    const metadata = {
        ...kwargs.config?.metadata,
        run_attempt: options.attempt,
        langgraph_version: LANGGRAPH_VERSION?.version ?? "0.0.0",
        langgraph_plan: "developer",
        langgraph_host: "self-hosted",
        langgraph_api_url: process.env.LANGGRAPH_API_URL ?? undefined,
    };
    const tracer = run.kwargs?.config?.configurable?.langsmith_project
        ? new LangChainTracer({
            replicas: [
                [
                    run.kwargs?.config?.configurable?.langsmith_project,
                    {
                        reference_example_id: run.kwargs?.config?.configurable?.langsmith_example_id,
                    },
                ],
                [getDefaultProjectName(), undefined],
            ],
        })
        : undefined;
    const events = graph.streamEvents(kwargs.command != null
        ? getLangGraphCommand(kwargs.command)
        : kwargs.input ?? null, {
        version: "v2",
        interruptAfter: kwargs.interrupt_after,
        interruptBefore: kwargs.interrupt_before,
        tags: kwargs.config?.tags,
        context: kwargs.context,
        configurable: kwargs.config?.configurable,
        recursionLimit: kwargs.config?.recursion_limit,
        subgraphs: kwargs.subgraphs,
        metadata,
        runId: run.run_id,
        streamMode: [...libStreamMode],
        signal: options?.signal,
        ...(tracer && { callbacks: [tracer] }),
    });
    const messages = {};
    const completedIds = new Set();
    for await (const event of events) {
        if (event.tags?.includes("langsmith:hidden"))
            continue;
        if (event.event === "on_chain_stream" && event.run_id === run.run_id) {
            const [ns, mode, chunk] = (kwargs.subgraphs ? event.data.chunk : [null, ...event.data.chunk]);
            // Listen for debug events and capture checkpoint
            let data = chunk;
            if (mode === "debug") {
                const debugChunk = chunk;
                if (debugChunk.type === "checkpoint") {
                    const debugCheckpoint = preprocessDebugCheckpoint(debugChunk.payload);
                    options?.onCheckpoint?.(debugCheckpoint);
                    data = { ...debugChunk, payload: debugCheckpoint };
                }
                else if (debugChunk.type === "task_result") {
                    const debugResult = preprocessDebugCheckpointTask(debugChunk.payload);
                    options?.onTaskResult?.(debugResult);
                    data = { ...debugChunk, payload: debugResult };
                }
            }
            else if (mode === "checkpoints") {
                const debugCheckpoint = preprocessDebugCheckpoint(chunk);
                options?.onCheckpoint?.(debugCheckpoint);
                data = debugCheckpoint;
            }
            else if (mode === "tasks") {
                const debugTask = preprocessDebugCheckpointTask(chunk);
                if ("result" in debugTask || "error" in debugTask) {
                    options?.onTaskResult?.(debugTask);
                }
                data = debugTask;
            }
            if (mode === "messages") {
                if (userStreamMode.includes("messages-tuple")) {
                    if (kwargs.subgraphs && ns?.length) {
                        yield { event: `messages|${ns.join("|")}`, data };
                    }
                    else {
                        yield { event: "messages", data };
                    }
                }
            }
            else if (userStreamMode.includes(mode)) {
                if (kwargs.subgraphs && ns?.length) {
                    yield { event: `${mode}|${ns.join("|")}`, data };
                }
                else {
                    yield { event: mode, data };
                }
            }
        }
        else if (userStreamMode.includes("events")) {
            yield { event: "events", data: event };
        }
        // TODO: we still rely on old messages mode based of streamMode=values
        // In order to fully switch to library messages mode, we need to do ensure that
        // `StreamMessagesHandler` sends the final message, which requires the following:
        // - handleLLMEnd does not send the final message b/c handleLLMNewToken sets the this.emittedChatModelRunIds[runId] flag. Python does not do that
        // - handleLLMEnd receives the final message as BaseMessageChunk rather than BaseMessage, which from the outside will become indistinguishable.
        // - handleLLMEnd should not dedupe the message
        // - Don't think there's an utility that would convert a BaseMessageChunk to a BaseMessage?
        if (userStreamMode.includes("messages")) {
            if (event.event === "on_chain_stream" && event.run_id === run.run_id) {
                const newMessages = [];
                const [_, chunk] = event.data.chunk;
                let chunkMessages = [];
                if (typeof chunk === "object" &&
                    chunk != null &&
                    "messages" in chunk &&
                    !isBaseMessage(chunk)) {
                    chunkMessages = chunk?.messages;
                }
                if (!Array.isArray(chunkMessages)) {
                    chunkMessages = [chunkMessages];
                }
                for (const message of chunkMessages) {
                    if (!message.id || completedIds.has(message.id))
                        continue;
                    completedIds.add(message.id);
                    newMessages.push(message);
                }
                if (newMessages.length > 0) {
                    yield { event: "messages/complete", data: newMessages };
                }
            }
            else if (event.event === "on_chat_model_stream" &&
                !event.tags?.includes("nostream")) {
                const message = event.data.chunk;
                if (!message.id)
                    continue;
                if (messages[message.id] == null) {
                    messages[message.id] = message;
                    yield {
                        event: "messages/metadata",
                        data: { [message.id]: { metadata: event.metadata } },
                    };
                }
                else {
                    messages[message.id] = messages[message.id].concat(message);
                }
                yield { event: "messages/partial", data: [messages[message.id]] };
            }
        }
    }
    if (kwargs.feedback_keys) {
        const client = new LangSmithClient();
        const data = Object.fromEntries(await Promise.all(kwargs.feedback_keys.map(async (feedback) => {
            const { url } = await client.createPresignedFeedbackToken(run.run_id, feedback);
            return [feedback, url];
        })));
        yield { event: "feedback", data };
    }
}
