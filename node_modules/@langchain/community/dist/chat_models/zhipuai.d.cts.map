{"version":3,"file":"zhipuai.d.cts","names":["BaseChatModel","BaseChatModelParams","BaseMessage","ChatGenerationChunk","ChatResult","CallbackManagerForLLMRun","ZhipuMessageRole","ZhipuMessage","ModelName","NonNullable","ChatCompletionRequest","ChatZhipuAIParams","ChatZhipuAI","Partial","Omit","Promise","AbortSignal","MessageEvent","AsyncGenerator"],"sources":["../../src/chat_models/zhipuai.d.ts"],"sourcesContent":["import { BaseChatModel, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { type CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nexport type ZhipuMessageRole = \"system\" | \"assistant\" | \"user\";\ninterface ZhipuMessage {\n    role: ZhipuMessageRole;\n    content: string;\n}\n/**\n * Interface representing a request for a chat completion.\n *\n * See https://open.bigmodel.cn/dev/howuse/model\n */\ntype ModelName = (string & NonNullable<unknown>) | \"chatglm_pro\" | \"chatglm_std\" | \"chatglm_lite\" | \"glm-4\" | \"glm-4v\" | \"glm-3-turbo\" | \"chatglm_turbo\";\ninterface ChatCompletionRequest {\n    model: ModelName;\n    messages?: ZhipuMessage[];\n    do_sample?: boolean;\n    stream?: boolean;\n    request_id?: string;\n    max_tokens?: number | null;\n    top_p?: number | null;\n    top_k?: number | null;\n    temperature?: number | null;\n    stop?: string[];\n}\n/**\n * Interface defining the input to the ZhipuAIChatInput class.\n */\nexport interface ChatZhipuAIParams {\n    /**\n     * @default \"glm-3-turbo\"\n     * Alias for `model`\n     */\n    modelName: ModelName;\n    /**\n     * @default \"glm-3-turbo\"\n     */\n    model: ModelName;\n    /** Whether to stream the results or not. Defaults to false. */\n    streaming?: boolean;\n    /** Messages to pass as a prefix to the prompt */\n    messages?: ZhipuMessage[];\n    /**\n     * API key to use when making requests. Defaults to the value of\n     * `ZHIPUAI_API_KEY` environment variable.\n     * Alias for `apiKey`\n     */\n    zhipuAIApiKey?: string;\n    /**\n     * API key to use when making requests. Defaults to the value of\n     * `ZHIPUAI_API_KEY` environment variable.\n     */\n    apiKey?: string;\n    /** Amount of randomness injected into the response. Ranges\n     * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /\n     * multiple choice, and temp closer to 1 for creative\n     * and generative tasks. Defaults to 0.95\n     */\n    temperature?: number;\n    /** Total probability mass of tokens to consider at each step. Range\n     * from 0 to 1 Defaults to 0.7\n     */\n    topP?: number;\n    /**\n     * Unique identifier for the request. Defaults to a random UUID.\n     */\n    requestId?: string;\n    /**\n     * turn on sampling strategy when do_sample is true,\n     * do_sample is false, temperature、top_p will not take effect\n     */\n    doSample?: boolean;\n    /**\n     * max value is 8192，defaults to 1024\n     */\n    maxTokens?: number;\n    stop?: string[];\n}\nexport declare class ChatZhipuAI extends BaseChatModel implements ChatZhipuAIParams {\n    static lc_name(): string;\n    get callKeys(): string[];\n    get lc_secrets(): {\n        zhipuAIApiKey: string;\n        apiKey: string;\n    };\n    get lc_aliases(): undefined;\n    zhipuAIApiKey?: string;\n    apiKey?: string;\n    streaming: boolean;\n    doSample?: boolean;\n    messages?: ZhipuMessage[];\n    requestId?: string;\n    modelName: ChatCompletionRequest[\"model\"];\n    model: ChatCompletionRequest[\"model\"];\n    apiUrl: string;\n    maxTokens?: number | undefined;\n    temperature?: number | undefined;\n    topP?: number | undefined;\n    stop?: string[];\n    constructor(fields?: Partial<ChatZhipuAIParams> & BaseChatModelParams);\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(): Omit<ChatCompletionRequest, \"messages\">;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<ChatCompletionRequest, \"messages\">;\n    /** @ignore */\n    _generate(messages: BaseMessage[], options?: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    /** @ignore */\n    completionWithRetry(request: ChatCompletionRequest, stream: boolean, signal?: AbortSignal, onmessage?: (event: MessageEvent) => void): Promise<any>;\n    private createZhipuStream;\n    private _deserialize;\n    _streamResponseChunks(messages: BaseMessage[], options?: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    _llmType(): string;\n    /** @ignore */\n    _combineLLMOutput(): never[];\n}\nexport {};\n//# sourceMappingURL=zhipuai.d.ts.map"],"mappings":";;;;;;KAIYM,gBAAAA;UACFC,YAAAA;EADED,IAAAA,EAEFA,gBAFkB;EAClBC,OAAAA,EAAAA,MAAY;AACI;AAQY;AAgBtC;;;;AAa2B,KA7BtBC,SAAAA,GA6BsB,CAAA,MAAA,GA7BAC,WA6BA,CAAA,OAAA,CAAA,CAAA,GAAA,aAAA,GAAA,aAAA,GAAA,cAAA,GAAA,OAAA,GAAA,QAAA,GAAA,aAAA,GAAA,eAAA;AAqC3B,UAjEUC,qBAAAA,CAiEsB;EAYjBH,KAAAA,EA5EJC,SA4EID;EAEAG,QAAAA,CAAAA,EA7EAH,YA6EAG,EAAAA;EACJA,SAAAA,CAAAA,EAAAA,OAAAA;EAMsBC,MAAAA,CAAAA,EAAAA,OAAAA;EAARE,UAAAA,CAAAA,EAAAA,MAAAA;EAA6BZ,UAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAIzBS,KAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAALI,KAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAIMJ,WAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAALI,IAAAA,CAAAA,EAAAA,MAAAA,EAAAA;;;;;AAIQJ,UAnFhBC,iBAAAA,CAmFgBD;EAAiDM;;;;EAGmBX,SAAAA,EAjFtFG,SAiFsFH;EAA0CF;;;EApC7EQ,KAAAA,EAzCvDH,SAyCuDG;EAAiB;;;aArCpEJ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAqCMK,WAAAA,SAAoBZ,aAAAA,YAAyBW;;;;;;;;;;;;aAYnDJ;;aAEAG;SACJA;;;;;;uBAMcG,QAAQF,qBAAqBV;;;;sBAI9Ba,KAAKJ;;;;uBAIJI,KAAKJ;;sBAENR,iEAAiEG,2BAA2BU,QAAQX;;+BAE3FM,iDAAiDM,iCAAiCC,wBAAwBF;;;kCAGvGb,iEAAiEG,2BAA2Ba,eAAef"}